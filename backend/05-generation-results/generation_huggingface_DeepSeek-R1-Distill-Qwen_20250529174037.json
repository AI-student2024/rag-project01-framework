{
  "query": "What is GRPO",
  "timestamp": "2025-05-29T17:40:37.847144",
  "provider": "huggingface",
  "model": "DeepSeek-R1-Distill-Qwen",
  "response": "[ ]\n</think>\n\nGRPO stands for Group Relative Policy Optimization, a method introduced by Shao et al. (2024) to reduce the training costs of reinforcement learning (RL). Instead of using a critic model of the same size as the policy model, GRPO estimates the baseline using group scores. For each question \\( q \\), it samples a group of outputs \\(\\{o_1, o_2, \\ldots, o_G\\}\\) from the old policy \\(\\pi_{old}\\) and optimizes the policy model \\(\\pi_\\theta\\) by maximizing the objective. This approach reduces the computational costs associated with maintaining a critic model that is typically the same size as the policy model.",
  "context": [
    {
      "text": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:",
      "score": 0.5254793167114258,
      "metadata": {
        "source": "DeepSeek_R1.pdf",
        "page": "5",
        "chunk": 41,
        "total_chunks": 180,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-02-14T16:48:03.946618"
      }
    }
  ]
}