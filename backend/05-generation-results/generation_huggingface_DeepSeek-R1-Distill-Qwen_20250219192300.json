{
  "query": "What is GRPO",
  "timestamp": "2025-02-19T19:23:00.023295",
  "provider": "huggingface",
  "model": "DeepSeek-R1-Distill-Qwen",
  "response": "基于提供的上下文，GRPO是一种在强化学习中用于减少训练成本的方法，通过通过评估旧策略的一个子集，然后优化新策略来实现。具体来说，GRPO根据每个问题的输出score，评估了一个组的旧策略输出，然后优化新策略，以最大化这些score的总和。因此，GRPO不需要与 critic模型同大小的政策模型，并且通过估计一个基于组的平均score来作为基线，从而减少训练的计算成本。这有助于提高训练效率和减少计算开销。\n\n**问题分析：**问题询问的是GRPO是什么。根据上下文中的内容，GRPO是一种在强化学习中减少训练成本的方法。具体来说，它通过评估旧策略的一个子集，然后优化新策略。此外，它不需要与 critic模型同大小的政策模型，并且通过估计一个基于组的平均score来作为基线，从而减少计算开销。因此，GRPO的定义是通过评估旧策略的一个子集，然后优化新策略，以最大化这些score的总和。",
  "context": [
    {
      "text": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:",
      "score": 0.5254793167114258,
      "metadata": {
        "source": "DeepSeek_R1.pdf",
        "page": "5",
        "chunk": 41,
        "total_chunks": 180,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-02-14T16:48:03.946618"
      }
    }
  ]
}