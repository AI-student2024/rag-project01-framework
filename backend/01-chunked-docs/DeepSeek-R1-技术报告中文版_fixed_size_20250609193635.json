{
  "filename": "DeepSeek-R1-技术报告中文版.pdf",
  "total_chunks": 75,
  "total_pages": 5,
  "loading_method": "pypdf",
  "chunking_method": "fixed_size",
  "chunking_config": {
    "chunk_size": 100,
    "chunk_overlap": 20,
    "min_chunk_size": 50,
    "max_chunk_size": 300,
    "semantic_threshold": 0.7
  },
  "timestamp": "2025-06-09T19:36:35.584896",
  "chunks": [
    {
      "content": "AIME 2024\n(Pass@1)Codeforces\n(Percentile)GPQA Diamond\n(Pass@1)MATH-500\n(Pass@1)MMLU",
      "metadata": {
        "chunk_id": 1,
        "page_number": 1,
        "page_range": "1",
        "word_count": 7
      }
    },
    {
      "content": "(Pass@1)MMLU\n(Pass@1)SWE-bench Verified\n(Resolved)020406080100\nAccuracy / Percentile (%)79.896.3",
      "metadata": {
        "chunk_id": 2,
        "page_number": 1,
        "page_range": "1",
        "word_count": 8
      }
    },
    {
      "content": "71.597.3\n90.8\n49.279.296.6\n75.796.4\n91.8\n48.972.690.6\n62.194.3\n87.4\n36.863.693.4\n60.090.0\n85.2\n41.6",
      "metadata": {
        "chunk_id": 3,
        "page_number": 1,
        "page_range": "1",
        "word_count": 12
      }
    },
    {
      "content": ".0DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3DeepSeek-R1 ：通过强化学习激励 LLMs",
      "metadata": {
        "chunk_id": 4,
        "page_number": 1,
        "page_range": "1",
        "word_count": 7
      }
    },
    {
      "content": "深度探索人 工 智 能\nresearch@deepseek.com\n摘要\n我们推出了第一代推理模型， DeepSeek-R1-Zero 和 DeepSeek-R1",
      "metadata": {
        "chunk_id": 5,
        "page_number": 1,
        "page_range": "1",
        "word_count": 10
      }
    },
    {
      "content": "。DeepSeek-R1-Zero 是一个通\n过大规模强化学习（ RL ）训练而成的模型，无需监督微调（ SFT ）作为初步步骤，展示了卓越\n的推理能力",
      "metadata": {
        "chunk_id": 6,
        "page_number": 1,
        "page_range": "1",
        "word_count": 8
      }
    },
    {
      "content": "。通过  RL ，DeepSeek-R1-Zero 自然涌现出许多强大且有趣的推理行为。然而，它也\n面临诸如可读性差和语言混合等挑战",
      "metadata": {
        "chunk_id": 7,
        "page_number": 1,
        "page_range": "1",
        "word_count": 5
      }
    },
    {
      "content": "。为了解决这些问题并进一步提升推理性能，我们推出了  \nDeepSeek-R1 ，它在 RL 之前引入了多阶段训练和冷启动数据",
      "metadata": {
        "chunk_id": 8,
        "page_number": 1,
        "page_range": "1",
        "word_count": 5
      }
    },
    {
      "content": "ek-R1 以及基于  Qwen 和 Llama 从 DeepSeek-R1 蒸馏出的六个密集模型（ 1.5B 、7B 、8B 、14B 、\n32B 、70B ）",
      "metadata": {
        "chunk_id": 9,
        "page_number": 1,
        "page_range": "1",
        "word_count": 16
      }
    },
    {
      "content": "1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001",
      "metadata": {
        "chunk_id": 10,
        "page_number": 2,
        "page_range": "2",
        "word_count": 43
      }
    },
    {
      "content": ". . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 11,
        "page_number": 2,
        "page_range": "2",
        "word_count": 26
      }
    },
    {
      "content": "2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002",
      "metadata": {
        "chunk_id": 12,
        "page_number": 2,
        "page_range": "2",
        "word_count": 46
      }
    },
    {
      "content": "上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5",
      "metadata": {
        "chunk_id": 13,
        "page_number": 2,
        "page_range": "2",
        "word_count": 25
      }
    },
    {
      "content": "2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 14,
        "page_number": 2,
        "page_range": "2",
        "word_count": 40
      }
    },
    {
      "content": ". . . . . . . . . . 6 2.2.3 训练模板  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 15,
        "page_number": 2,
        "page_range": "2",
        "word_count": 44
      }
    },
    {
      "content": "2.3 DeepSeek-R1 ：冷启动强化学习  . . . . . . . . . . . . . . . 9  \u00002.3.1 冷启动 . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 16,
        "page_number": 2,
        "page_range": "2",
        "word_count": 35
      }
    },
    {
      "content": ". . . . . . . . . . . . . . . 9  \u00002.3.2 面向推理的强化学习  . . . . . . . . . . . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 17,
        "page_number": 2,
        "page_range": "2",
        "word_count": 42
      }
    },
    {
      "content": "采样与监督微调  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.4 全场景强化学习  . . . . . . .",
      "metadata": {
        "chunk_id": 18,
        "page_number": 2,
        "page_range": "2",
        "word_count": 39
      }
    },
    {
      "content": ". . . . . . . . . . . . . . . 11  \u00002.4 蒸馏：赋予小模型推理能力  . . . . . . . . . . . . . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 19,
        "page_number": 2,
        "page_range": "2",
        "word_count": 41
      }
    },
    {
      "content": "3.1 DeepSeek-R1 评估 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3",
      "metadata": {
        "chunk_id": 20,
        "page_number": 2,
        "page_range": "2",
        "word_count": 38
      }
    },
    {
      "content": ". . . . . . . . 12 3.2 蒸馏模型评估  . . . . . . . . . .",
      "metadata": {
        "chunk_id": 21,
        "page_number": 2,
        "page_range": "2",
        "word_count": 21
      }
    },
    {
      "content": ". . . . . . . . . . . . . . . . . . . . . 14\n4 讨论 14",
      "metadata": {
        "chunk_id": 22,
        "page_number": 2,
        "page_range": "2",
        "word_count": 25
      }
    },
    {
      "content": "4.1 蒸馏与强化学习  . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 未成功的尝试  . . . . . . . . . . .",
      "metadata": {
        "chunk_id": 23,
        "page_number": 2,
        "page_range": "2",
        "word_count": 40
      }
    },
    {
      "content": ". . . . . . . . . . . . . 15\n5 结论、局限性与未来工作  16\nA 贡献与致谢  20\n2",
      "metadata": {
        "chunk_id": 24,
        "page_number": 2,
        "page_range": "2",
        "word_count": 21
      }
    },
    {
      "content": "1. 引言\n近年来，大型语言模型（ LLMs ）经历了快速的迭代和进化（ Anthropic, 2024; Google, 2024; Ope",
      "metadata": {
        "chunk_id": 25,
        "page_number": 3,
        "page_range": "3",
        "word_count": 10
      }
    },
    {
      "content": "。\n最近，后训练已成为完整训练流程中的一个重要组成部分。它已被证明可以提高推理任务的\n准确性，与社会价值观保持一致，并适应用户偏好，同时相对于预训练所需的计算资源相对较\n少",
      "metadata": {
        "chunk_id": 26,
        "page_number": 3,
        "page_range": "3",
        "word_count": 4
      }
    },
    {
      "content": "。在推理能力方面， OpenAI 的o1 （OpenAI, 2024b ）系列模型首次通过增加思维链推理过程的\n长度引入了推理时扩展。这种方法在数学、编码和科学推理等各种推理任务中取得了显著改进",
      "metadata": {
        "chunk_id": 27,
        "page_number": 3,
        "page_range": "3",
        "word_count": 7
      }
    },
    {
      "content": "。之前的一些工作探索了各种方法\n，包括基于过程的奖励模型（ Lightman 等，2023 ；Uesato 等，2022 ；Wang 等，2023 ）、强化学",
      "metadata": {
        "chunk_id": 28,
        "page_number": 3,
        "page_range": "3",
        "word_count": 9
      }
    },
    {
      "content": "习（Kumar 等，2024 ）以及蒙特卡洛树搜索和束搜索等搜索算法（ Feng 等，2024 ；Trinh 等，202\n4 ；Xin 等，2024 ）",
      "metadata": {
        "chunk_id": 29,
        "page_number": 3,
        "page_range": "3",
        "word_count": 11
      }
    },
    {
      "content": "。然而，这些方法都没有达到与 OpenAI 的o1 系列模型相媲美的通用推理性能\n。\n在本文中，我们迈出了利用纯强化学习（ RL ）提升语言模型推理能力的第一步",
      "metadata": {
        "chunk_id": 30,
        "page_number": 3,
        "page_range": "3",
        "word_count": 8
      }
    },
    {
      "content": "。我们的目标\n是探索大型语言模型（ LLMs ）在没有监督数据的情况下发展推理能力的潜力，重点关注它们通\n过纯RL 过程的自我进化",
      "metadata": {
        "chunk_id": 31,
        "page_number": 3,
        "page_range": "3",
        "word_count": 6
      }
    },
    {
      "content": "。具体而言，我们使用 DeepSeek-V3-Base 作为基础模型，并采用 GRPO （\nShao 等，2024 ）作为RL 框架，以提升模型在推理任务中的表现",
      "metadata": {
        "chunk_id": 32,
        "page_number": 3,
        "page_range": "3",
        "word_count": 9
      }
    },
    {
      "content": "。在训练过程中， DeepSeek-R1-Z\nero 自然涌现出许多强大且有趣的推理行为。经过数千次 RL 步骤后， DeepSeek-R1-Zero 在推理基\n准测试中展现出卓越的性能",
      "metadata": {
        "chunk_id": 33,
        "page_number": 3,
        "page_range": "3",
        "word_count": 9
      }
    },
    {
      "content": "。例如， AIME 2024 上的pass@1 得分从15.6% 提升至71.0% ，而在多\n数投票机制下，得分进一步提高至 86.7% ，与OpenAI-o1-0912 的性能相当",
      "metadata": {
        "chunk_id": 34,
        "page_number": 3,
        "page_range": "3",
        "word_count": 11
      }
    },
    {
      "content": "。\n然而，DeepSeek-R1-Zero 遇到了诸如可读性差和语言混合等挑战。为了解决这些问题并进一\n步提升推理性能，我们引入了  DeepSeek-R1 ，它结合了少量冷启动数据和多阶段训练流程",
      "metadata": {
        "chunk_id": 35,
        "page_number": 3,
        "page_range": "3",
        "word_count": 6
      }
    },
    {
      "content": "。具体\n来说，我们首先收集数千条冷启动数据来微调  DeepSeek-V3-Base 模型。随后，我们进行类似  De\nepSeek-R1-Zero 的推理导向强化学习（ RL ）",
      "metadata": {
        "chunk_id": 36,
        "page_number": 3,
        "page_range": "3",
        "word_count": 9
      }
    },
    {
      "content": "。在 RL 过程接近收敛时，我们通过对  RL 检查点进\n行拒绝采样，结合来自  DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，创建新",
      "metadata": {
        "chunk_id": 37,
        "page_number": 3,
        "page_range": "3",
        "word_count": 8
      }
    },
    {
      "content": "。使用新数据微调后，检查点会经历额外的  \nRL 过程，考虑所有场景的提示。经过这些步骤，我们获得了称为  DeepSeek-R1 的检查点，其性\n能与 OpenAI-o1-1217 相当",
      "metadata": {
        "chunk_id": 38,
        "page_number": 3,
        "page_range": "3",
        "word_count": 8
      }
    },
    {
      "content": "。以 Qwen2.5-32B （Qwen, 2024\nb ）为基础模型，直接从 DeepSeek-R1 进行蒸馏的效果优于在其上应用强化学习",
      "metadata": {
        "chunk_id": 39,
        "page_number": 3,
        "page_range": "3",
        "word_count": 8
      }
    },
    {
      "content": "。这表明，更大\n基础模型发现的推理模式对于提升推理能力至关重要。我们开源了蒸馏后的 Qwen 和Llama （Dub\ney 等，2024 ）系列",
      "metadata": {
        "chunk_id": 40,
        "page_number": 3,
        "page_range": "3",
        "word_count": 8
      }
    },
    {
      "content": "-Preview （Qwen, 2024a ），而蒸馏的 32B 和70B 模型在密集模型的推理基准测试中创下了新纪录",
      "metadata": {
        "chunk_id": 41,
        "page_number": 3,
        "page_range": "3",
        "word_count": 7
      }
    },
    {
      "content": "1.1. 贡献\n训练后：在基础模型上进行大规模强化学习\n• 我们直接将强化学习（ RL ）应用于基础模型，而不依赖于监督微调（ SFT ）作为初步步骤",
      "metadata": {
        "chunk_id": 42,
        "page_number": 4,
        "page_range": "4",
        "word_count": 9
      }
    },
    {
      "content": "。这种方法使模型能够探索思维链（ CoT ）以解决复杂问题，从而开发出 DeepSeek-R1-Zer\no",
      "metadata": {
        "chunk_id": 43,
        "page_number": 4,
        "page_range": "4",
        "word_count": 5
      }
    },
    {
      "content": "。DeepSeek-R1-Zero 展示了自我验证、反思和生成长思维链等能力，标志着研究社区的一\n个重要里程碑",
      "metadata": {
        "chunk_id": 44,
        "page_number": 4,
        "page_range": "4",
        "word_count": 3
      }
    },
    {
      "content": "。值得注意的是，这是首个公开的研究，验证了 LLMs 的推理能力可以纯粹\n通过RL 激励，而不需要 SFT 。这一突破为这一领域的未来进展铺平了道路",
      "metadata": {
        "chunk_id": 45,
        "page_number": 4,
        "page_range": "4",
        "word_count": 7
      }
    },
    {
      "content": "。该流程包含两个 RL 阶段，旨在发现改进的推理模式\n并与人类偏好对齐，以及两个 SFT 阶段，作为模型推理和非推理能力的基础。我们相信该\n流程将通过创建更好的模型为行业带来益处",
      "metadata": {
        "chunk_id": 46,
        "page_number": 4,
        "page_range": "4",
        "word_count": 7
      }
    },
    {
      "content": "。\n蒸馏：小型模型也能强大\n• 我们证明了较大模型的推理模式可以被提炼到较小的模型中，与通过强化学习在小模型上\n发现的推理模式相比，性能更优",
      "metadata": {
        "chunk_id": 47,
        "page_number": 4,
        "page_range": "4",
        "word_count": 5
      }
    },
    {
      "content": "。开源的 DeepSeek-R1 及其API 将有益于研究社区在未来提\n炼出更好的小型模型。\n• 利用DeepSeek-R1 生成的推理数据，我们对研究社区中广泛使用的多个密集模型进行了微\n调",
      "metadata": {
        "chunk_id": 48,
        "page_number": 4,
        "page_range": "4",
        "word_count": 9
      }
    },
    {
      "content": "。 DeepSeek-R1-\nDistill-Qwen-7B 在AIME 2024 上达到了 55.5% ，超越了 QwQ-32B-Preview",
      "metadata": {
        "chunk_id": 49,
        "page_number": 4,
        "page_range": "4",
        "word_count": 9
      }
    },
    {
      "content": "。此外， DeepSeek-\nR1-Distill-Qwen-32B 在AIME 2024 上得分为 72.6% ，在MATH-500 上得分为 94.3% ，在LiveC",
      "metadata": {
        "chunk_id": 50,
        "page_number": 4,
        "page_range": "4",
        "word_count": 11
      }
    },
    {
      "content": "。这些结果显著优于之前的开源模型，并与 o1-mini 相当。我们向\n社区开源了基于 Qwen2.5 和Llama3 系列的1.5B 、7B 、8B 、14B 、32B 和70B 蒸馏检查点",
      "metadata": {
        "chunk_id": 51,
        "page_number": 4,
        "page_range": "4",
        "word_count": 13
      }
    },
    {
      "content": "。\n1.2. 评估结果总结\n• 推理任务：（ 1 ）DeepSeek-R1 在 AIME 2024 上取得了  79.8% 的 Pass@1 分数，略微超过\n了 OpenAI-o1-1217",
      "metadata": {
        "chunk_id": 52,
        "page_number": 4,
        "page_range": "4",
        "word_count": 17
      }
    },
    {
      "content": "。在 MATH-500 上，它获得了令人印象深刻的  97.3% 的分数，与  Open\nAI-o1-1217 表现相当，并显著优于其他模型",
      "metadata": {
        "chunk_id": 53,
        "page_number": 4,
        "page_range": "4",
        "word_count": 8
      }
    },
    {
      "content": "。（ 2 ）在与编码相关的任务中， DeepSeek-R1 \n在代码竞赛任务中展示了专家水平，它在  Codeforces 上获得了  2,029 的 Elo 评分，超过了",
      "metadata": {
        "chunk_id": 54,
        "page_number": 4,
        "page_range": "4",
        "word_count": 11
      }
    },
    {
      "content": "。对于工程相关任务， DeepSeek-R1 的表现略优于  DeepSeek-V3 ，这\n可能有助于开发人员在现实世界任务中取得更好的成果",
      "metadata": {
        "chunk_id": 55,
        "page_number": 4,
        "page_range": "4",
        "word_count": 6
      }
    },
    {
      "content": "。\n• 知识：在 MMLU 、MMLU-Pro 和GPQA Diamond 等基准测试中， DeepSeek-R1 取得了出色的",
      "metadata": {
        "chunk_id": 56,
        "page_number": 4,
        "page_range": "4",
        "word_count": 10
      }
    },
    {
      "content": "成绩，显著超越了 DeepSeek-V3 ，得分分别为 MMLU 90.8% 、MMLU-Pro 84.0% 和GPQA Di\namond 71.5%",
      "metadata": {
        "chunk_id": 57,
        "page_number": 4,
        "page_range": "4",
        "word_count": 11
      }
    },
    {
      "content": "。虽然在这些基准测试中其表现略低于 OpenAI-o1-1217 ，但DeepSeek-R1 超越\n了其他闭源模型，展示了其在教育任务中的竞争优势",
      "metadata": {
        "chunk_id": 58,
        "page_number": 4,
        "page_range": "4",
        "word_count": 5
      }
    },
    {
      "content": "。在事实基准测试 SimpleQA 上，Dee\npSeek-R1 的表现优于 DeepSeek-V3 ，展示了其处理基于事实的查询的能力",
      "metadata": {
        "chunk_id": 59,
        "page_number": 4,
        "page_range": "4",
        "word_count": 7
      }
    },
    {
      "content": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等",
      "metadata": {
        "chunk_id": 60,
        "page_number": 5,
        "page_range": "5",
        "word_count": 5
      }
    },
    {
      "content": "。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力",
      "metadata": {
        "chunk_id": 61,
        "page_number": 5,
        "page_range": "5",
        "word_count": 11
      }
    },
    {
      "content": "。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3",
      "metadata": {
        "chunk_id": 62,
        "page_number": 5,
        "page_range": "5",
        "word_count": 5
      }
    },
    {
      "content": "。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力",
      "metadata": {
        "chunk_id": 63,
        "page_number": 5,
        "page_range": "5",
        "word_count": 10
      }
    },
    {
      "content": "。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长",
      "metadata": {
        "chunk_id": 64,
        "page_number": 5,
        "page_range": "5",
        "word_count": 11
      }
    },
    {
      "content": "链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中",
      "metadata": {
        "chunk_id": 65,
        "page_number": 5,
        "page_range": "5",
        "word_count": 8
      }
    },
    {
      "content": "。\n2.2. DeepSeek-R1-Zero ：基础模型上的强化学习\n强化学习在推理任务中展现了显著的有效性，正如我们之前的工作所证明的那样（ Shao 等，202\n4 ；Wang 等，2023 ）",
      "metadata": {
        "chunk_id": 66,
        "page_number": 5,
        "page_range": "5",
        "word_count": 11
      }
    },
    {
      "content": "。然而，这些工作严重依赖于监督数据，而这些数据的收集非常耗时。在\n本节中，我们探索了大型语言模型（ LLMs ）在没有监督数据的情况下发展推理能力的潜力，重\n点关注它们通过纯强化学习过程的自我进化",
      "metadata": {
        "chunk_id": 67,
        "page_number": 5,
        "page_range": "5",
        "word_count": 5
      }
    },
    {
      "content": "。我们首先简要概述了我们的强化学习算法，随后\n展示了一些令人兴奋的结果，并希望这能为社区提供有价值的见解",
      "metadata": {
        "chunk_id": 68,
        "page_number": 5,
        "page_range": "5",
        "word_count": 2
      }
    },
    {
      "content": "。\n2.2.1.ReinforcementLearningAlgorithm\n群体相对策略优化  为了节省强化学习的训练成本，我们采用了群体相对策略优化（ GRPO ）（S",
      "metadata": {
        "chunk_id": 69,
        "page_number": 5,
        "page_range": "5",
        "word_count": 6
      }
    },
    {
      "content": "hao 等，2024 ），该方法放弃了通常与策略模型大小相同的评论家模型，转而从群体分数中估计\n基线",
      "metadata": {
        "chunk_id": 70,
        "page_number": 5,
        "page_range": "5",
        "word_count": 4
      }
    },
    {
      "content": "。具体来说，对于每个问题 𝑞 ，GRPO 从旧策略𝜋𝜃𝑜𝑙𝑑 中采样一组输出{𝑜1 、𝑜2 、··· 、𝑜𝐺} ，然后\n通过最大化以下目标来优化策略模型 𝜋𝜃 ：",
      "metadata": {
        "chunk_id": 71,
        "page_number": 5,
        "page_range": "5",
        "word_count": 12
      }
    },
    {
      "content": "J𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺\n𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\n1\n𝐺𝐺∑︁\n𝑖=1\u0012\nmin\u0012𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞)𝐴𝑖,clip\u0012𝜋𝜃(𝑜𝑖|𝑞)",
      "metadata": {
        "chunk_id": 72,
        "page_number": 5,
        "page_range": "5",
        "word_count": 7
      }
    },
    {
      "content": "𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞),1−𝜀,1+𝜀\u0013\n𝐴𝑖\u0013\n−𝛽D𝐾𝐿\u0000\n𝜋𝜃||𝜋𝑟𝑒𝑓\u0001\u0013\n,(1)\nD𝐾𝐿\u0000\n𝜋𝜃||𝜋𝑟𝑒𝑓\u0001=𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞)−log𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)",
      "metadata": {
        "chunk_id": 73,
        "page_number": 5,
        "page_range": "5",
        "word_count": 8
      }
    },
    {
      "content": "𝜋𝜃(𝑜𝑖|𝑞)−1, (2)\n其中 𝜀 和 𝛽 是超参数， 𝐴𝑖 是优势，通过使用一组奖励  {𝑟1 、𝑟2 、... 、𝑟𝐺} 来计算，这些奖励对应\n于每个组内的输出：",
      "metadata": {
        "chunk_id": 74,
        "page_number": 5,
        "page_range": "5",
        "word_count": 15
      }
    },
    {
      "content": "于每个组内的输出：\n𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\ns𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n5",
      "metadata": {
        "chunk_id": 75,
        "page_number": 5,
        "page_range": "5",
        "word_count": 5
      }
    }
  ]
}